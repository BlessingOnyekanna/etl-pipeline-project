# ğŸ”„ ETL Pipeline - Production-Ready Data Integration System

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A comprehensive, modular ETL (Extract, Transform, Load) pipeline built for real-world data engineering scenarios. This project demonstrates production-ready data integration from multiple sources with robust error handling, logging, and configuration management.

## ğŸ¯ Overview

This ETL pipeline simulates a real-world retail analytics scenario where data flows from multiple sources into a centralized analytics database.

**Use Case**: A retail company collecting daily sales data from their Shopify API, inventory data from MySQL, customer data from PostgreSQL, and shipping data from CSV files. The pipeline extracts, cleans, merges, and loads this data into an analytics database for reporting.

## âœ¨ Features

### Extract Component
- âœ… CSV file extraction with configurable delimiters
- âœ… REST API integration with retry logic
- âœ… MySQL database extraction
- âœ… PostgreSQL database extraction
- âœ… Modular, reusable extractors

### Transform Component
- âœ… Automated data cleaning (missing values, duplicates, outliers)
- âœ… Data type validation and conversion
- âœ… Multi-source data merging
- âœ… Data quality reporting
- âœ… Configurable transformation rules

### Load Component
- âœ… PostgreSQL loader with auto-table creation
- âœ… MySQL loader with schema management
- âœ… CSV export functionality
- âœ… Cloud storage simulation (local folders)
- âœ… Flexible load strategies (replace, append, fail)

### Pipeline Features
- âœ… YAML-based configuration (no code changes needed)
- âœ… Comprehensive logging system
- âœ… Error handling with retry logic
- âœ… Pipeline orchestration
- âœ… Performance monitoring
- âœ… Modular, maintainable architecture

## ğŸ“ Project Structure
```
etl_pipeline_project/
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ pipeline_config.yaml          # Main configuration file
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extractors/                    # Data extraction modules
â”‚   â”œâ”€â”€ transformers/                  # Data transformation modules
â”‚   â”œâ”€â”€ loaders/                       # Data loading modules
â”‚   â”œâ”€â”€ utils/                         # Utility modules
â”‚   â”‚   â”œâ”€â”€ logger.py                 # Logging system
â”‚   â”‚   â”œâ”€â”€ config_loader.py          # Configuration management
â”‚   â”‚   â””â”€â”€ db_connection.py          # Database utilities
â”‚   â””â”€â”€ pipeline.py                    # Main pipeline orchestrator
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                           # Input data files
â”‚   â”œâ”€â”€ processed/                     # Output data files
â”‚   â””â”€â”€ cloud_storage/                 # Simulated cloud storage
â”‚
â”œâ”€â”€ logs/                              # Pipeline execution logs
â”œâ”€â”€ reports/                           # Data quality reports
â””â”€â”€ requirements.txt                   # Python dependencies
```

## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- PostgreSQL 12+ (optional, for database sources/destinations)
- MySQL 8+ (optional, for database sources/destinations)

### Setup

1. **Clone the repository**
```bash
git clone https://github.com/BlessingOnyekanna/etl_pipeline_project.git
cd etl_pipeline_project
```

2. **Create virtual environment**

**Windows (PowerShell):**
```powershell
python -m venv venv
.\venv\Scripts\Activate.ps1
```

**Linux/Mac:**
```bash
python3 -m venv venv
source venv/bin/activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Configure the pipeline**
Edit `config/pipeline_config.yaml` with your settings:
- Database credentials
- File paths
- API endpoints
- Transformation rules

## âš¡ Quick Start

### Basic Pipeline Run
```bash
python src/pipeline.py
```

### Run with Custom Configuration
```bash
python src/pipeline.py --config config/my_pipeline_config.yaml
```

### Test Individual Components
```python
# Test configuration loading
python src/utils/config_loader.py

# Test logging
python src/utils/logger.py
```

## âš™ï¸ Configuration

The pipeline is controlled entirely through `config/pipeline_config.yaml`:
```yaml
# Example: Enable/disable data sources
sources:
  csv:
    enabled: true
    file_path: "data/raw/shipping_data.csv"
    
  api:
    enabled: true
    base_url: "https://fakestoreapi.com"

# Example: Configure transformations
transform:
  missing_values:
    numeric_strategy: "mean"  # mean, median, zero, drop
    categorical_strategy: "mode"  # mode, unknown, drop

# Example: Set destinations
destinations:
  postgresql_analytics:
    enabled: true
    database: "analytics_db"
    if_exists: "replace"  # replace, append, fail
```

## ğŸ“Š What This Demonstrates

### Technical Skills
- Python development
- YAML configuration
- Database connectivity (MySQL, PostgreSQL)
- REST API integration
- Data manipulation with Pandas
- Error handling and logging
- Professional documentation

### Professional Practices
- Modular, maintainable code
- Configuration-driven design
- Comprehensive logging
- Production-ready patterns
- Enterprise-grade error handling

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“ License

This project is licensed under the MIT License.

## ğŸ‘¤ Author

**Blessing Onyekanna**
- GitHub: [@BlessingOnyekanna](https://github.com/BlessingOnyekanna)

## ğŸ™ Acknowledgments

- Built as part of IBM Data Engineering Professional Certificate
- Inspired by real-world data engineering challenges

## ğŸ“š Related Projects

- [Insight Generator](https://github.com/BlessingOnyekanna/Insight-generator) - E-commerce analytics with PDF reports
- [Data Doctor](https://github.com/BlessingOnyekanna/Data-cleaning-portfolio) - Data cleaning and validation system

---

**â­ If you find this project useful, please consider giving it a star!**